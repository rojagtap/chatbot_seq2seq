{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHjvEsSO6C-n",
        "colab_type": "code",
        "outputId": "a20f7b45-8822-4b66-8200-e51be36542bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et_KyuIQ5h4M",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-processed data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrLDYLL-5h4N",
        "colab_type": "code",
        "outputId": "71bda0ac-06dc-4203-87d2-a682fcb284ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, CuDNNLSTM, Dense, TimeDistributed, LSTM"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UoIfLfj5h4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading X and Y\n",
        "with open('/content/drive/My Drive/Colab Notebooks/chatbot_seq2seq/data/x_and_y.pkl', 'rb') as f:\n",
        "    X_encoder, X_decoder, y = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSPLXs7_5h4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading vocab_embeddings\n",
        "with open('/content/drive/My Drive/Colab Notebooks/chatbot_seq2seq/data/embedding_weights.pkl', 'rb') as f:\n",
        "    embedding_weights = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHuS0add5h4W",
        "colab_type": "text"
      },
      "source": [
        "#### Now the structure of our model will be like this:\n",
        "##### 1. Encoder LSTM will take the input (Embedded) and after processing, it will pass on the cell state to the decoder\n",
        "##### 2. Decoder LSTM will take initial state from cell state of encoder and inputs will be the Expected output just 1 word behind so basically the decoder lstm will predict the next word in the output sequence\n",
        "##### 3. A dense layer i.e a regular Feedforward NN will then predict the words occuring in the expected sentence in one-hot encoded form i.e the labels will be \"1\" where the word from the vocabulary is present in the given sentence and the predicted output will be in softmax probability form so basically prediction by -ve log loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhRyGm0L5h4X",
        "colab_type": "text"
      },
      "source": [
        "### Building the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWqpOc1L5h4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 20\n",
        "vocab_size = 15000\n",
        "embedding_dim = 300\n",
        "hidden_dim = 300\n",
        "# obtained in vocab_embedding\n",
        "number_of_samples = 221616\n",
        "# train : val = 93.75 %\n",
        "number_of_train_samples = 221616\n",
        "number_of_val_samples = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18mSZnFr5h4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 1026"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLFx9BpR5h4Z",
        "colab_type": "code",
        "outputId": "6c1b6a45-39a8-4ca4-b9cd-66f3813795ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Embedding Layer\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size, \n",
        "    output_dim=embedding_dim,\n",
        "    input_length=max_len,\n",
        "    weights=[embedding_weights],\n",
        "    trainable=False\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 04:40:46.456438 140022985574272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_KvMaZw5h4b",
        "colab_type": "code",
        "outputId": "792f0baf-b44d-4f1c-fc48-ad1bfd3ffbdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "# Encoder\n",
        "encoder_inputs = Input(batch_shape=(batch_size, max_len,), dtype='int32')\n",
        "encoder_embedding = embedding_layer(encoder_inputs)\n",
        "encoder_LSTM = CuDNNLSTM(hidden_dim, return_state=True, stateful=True)\n",
        "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0627 04:40:46.513562 140022985574272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 04:40:46.527290 140022985574272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0627 04:40:46.538114 140022985574272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0627 04:40:46.538940 140022985574272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_XwH5eg5h4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_len,), dtype='int32')\n",
        "decoder_embedding = embedding_layer(decoder_inputs)\n",
        "decoder_LSTM = CuDNNLSTM(hidden_dim, return_state=True, return_sequences=True)\n",
        "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X8AHmcw5h4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Output\n",
        "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Q3ZkkP5h4i",
        "colab_type": "code",
        "outputId": "f332d55c-b026-4e53-c00e-b4059edcdb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            (1026, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         multiple             4500000     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)        [(1026, 300), (1026, 722400      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)        [(None, 20, 300), (N 722400      embedding_1[1][0]                \n",
            "                                                                 cu_dnnlstm_1[0][1]               \n",
            "                                                                 cu_dnnlstm_1[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 20, 15000)    4515000     cu_dnnlstm_2[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 10,459,800\n",
            "Trainable params: 5,959,800\n",
            "Non-trainable params: 4,500,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaC5IgId5h4l",
        "colab_type": "code",
        "outputId": "bf8050d6-1ddd-4f88-e171-0835fa32dddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.compile(optimizer='sgd', loss ='categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0627 04:40:53.202728 140022985574272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbCGbH05h4n",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIz1gzLz5h4o",
        "colab_type": "text"
      },
      "source": [
        "#### Preparing train and val generator for training in batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPegStAl5h4o",
        "colab_type": "text"
      },
      "source": [
        "##### Making Y in one-hot encoded form first\n",
        "##### output is (number of sequences, max_len, vocab_size) i.e for each sentence at each position in maxlen what is the one hot encoding of the word present?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZsTOfux5h4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_generator(batch_size):\n",
        "    number_of_batches = int(number_of_train_samples / batch_size)\n",
        "    Y = np.zeros(shape=(batch_size, max_len, vocab_size), dtype=\"float32\")\n",
        "    while True:\n",
        "        for count in range(number_of_batches):\n",
        "            for i, sequences in enumerate(y[count * batch_size: count * batch_size + batch_size]):\n",
        "                for j, sequence in enumerate(sequences):\n",
        "                    Y[i][j][sequence] = 1\n",
        "            yield ([\n",
        "                X_encoder[count * batch_size: count * batch_size + batch_size],\n",
        "                X_decoder[count * batch_size: count * batch_size + batch_size]], \n",
        "                Y\n",
        "            )\n",
        "            Y.fill(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMmKowDN5h4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_generator(val_size):\n",
        "    if val_size == 0:\n",
        "        return None\n",
        "    validation_index = number_of_samples - val_size\n",
        "    Y = np.zeros(shape=(val_size, max_len, vocab_size), dtype=\"float32\")\n",
        "    for i, sequences in enumerate(y[validation_index:]):\n",
        "        for j, sequence in enumerate(sequences):\n",
        "            Y[i][j][sequence] = 1\n",
        "            \n",
        "    return ([X_encoder[validation_index:], X_decoder[validation_index:]], Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsNOD6kl5h4w",
        "colab_type": "code",
        "outputId": "64ed6ddb-3ade-4561-a81f-44b90fd13538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "train_gen = train_generator(batch_size)   # we have 513 batches of 405 samples (513 x 405 => 207765 + 13851 => 221616)\n",
        "val_gen = validation_generator(number_of_val_samples)\n",
        "\n",
        "# history is used for plotting\n",
        "history = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    steps_per_epoch=int(number_of_train_samples/batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=val_gen,\n",
        "    shuffle=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0627 04:40:53.364301 140022985574272 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "216/216 [==============================] - 216s 1s/step - loss: 4.2917 - acc: 0.7507\n",
            "Epoch 2/50\n",
            "216/216 [==============================] - 212s 982ms/step - loss: 2.8000 - acc: 0.7786\n",
            "Epoch 3/50\n",
            "216/216 [==============================] - 212s 981ms/step - loss: 2.3425 - acc: 0.7767\n",
            "Epoch 4/50\n",
            "216/216 [==============================] - 212s 980ms/step - loss: 2.2682 - acc: 0.7783\n",
            "Epoch 5/50\n",
            "216/216 [==============================] - 212s 980ms/step - loss: 2.2190 - acc: 0.7774\n",
            "Epoch 6/50\n",
            " 41/216 [====>.........................] - ETA: 2:51 - loss: 2.2088 - acc: 0.7750"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeMN70Z_5h4y",
        "colab_type": "text"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q5gowlA5h4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model Architecture\n",
        "\n",
        "# save as JSON\n",
        "json_string = model.to_json()\n",
        "open('model_architecture.json', 'w').write(json_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPL42Qso5h43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save the whole model\n",
        "\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZa_R3io5h45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model weights\n",
        "\n",
        "model.save_weights('model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUCnu7zJ5h49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho5ys6cY5h4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GEGUwpj5h5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}