{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "##### Dataset: Cornell Movie Dialogues\n",
    "- movie_conversations.txt (the structure of the conversations)\n",
    "\t- fields\n",
    "\t\t- characterID of the first character involved in the conversation\n",
    "\t\t- characterID of the second character involved in the conversation\n",
    "\t\t- movieID of the movie in which the conversation occurred\n",
    "\t\t- list of the utterances that make the conversation, in chronological \n",
    "\t\t\torder: ['lineID1','lineID2',...,'lineIDN']\n",
    "\t\t\thas to be matched with movie_lines.txt to reconstruct the actual content\n",
    "            \n",
    "            \n",
    "- movie_lines.txt (contains the actual text of each utterance)\n",
    "\t- fields:\n",
    "\t\t- lineID\n",
    "\t\t- characterID (who uttered this phrase)\n",
    "\t\t- movieID\n",
    "\t\t- character name\n",
    "\t\t- text of the utterance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"cornell_movie-dialogs_corpus/movie_conversations.txt\", \"cornell_movie-dialogs_corpus/movie_lines.txt\"]\n",
    "df = []\n",
    "for path in paths:\n",
    "    file = open(path)\n",
    "    lines = []\n",
    "    for line in file.readlines():\n",
    "        lines.append(line.replace('\\n', \"\").split(\" +++$+++ \"))\n",
    "    df.append(pd.DataFrame(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df[0] is movie conversations (\"Dataframe of\" \"list of\" dialogues (lineID) in a conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['L194', 'L195', 'L196', 'L197']\n",
       "1                    ['L198', 'L199']\n",
       "2    ['L200', 'L201', 'L202', 'L203']\n",
       "3            ['L204', 'L205', 'L206']\n",
       "4                    ['L207', 'L208']\n",
       "Name: dialogues_list, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning the column titles to the df\n",
    "df[0].columns = ['characterID_1', 'characterID_2', 'movieID', 'dialogues_list']\n",
    "\n",
    "# reducing the dataframe to required fields\n",
    "df[0] = df[0]['dialogues_list']\n",
    "df[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df[1] is movie lines (Dataframe of line ID and Dialogue text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lineID</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lineID      dialogue\n",
       "0  L1045  They do not!\n",
       "1  L1044   They do to!\n",
       "2   L985    I hope so.\n",
       "3   L984     She okay?\n",
       "4   L925     Let's go."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning the column titles to the df\n",
    "df[1].columns = ['lineID', 'characterID', 'movieID', 'character_name', 'dialogue']\n",
    "\n",
    "# reducing the dataframe to required fields\n",
    "df[1] = df[1][['lineID', 'dialogue']]\n",
    "df[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Following 2 cells will set basis for passing data in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value of df[0] is in the form of list, but it returns a string, so replace characters ', [, ], space with empty string and make a list of 'lineIDs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L8200', 'L8201', 'L8202']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df[0].iloc[1000].replace(\"\\\"\", \"\").replace(\"\\'\", \"\").replace('[', \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### df[0] has conversations but the lines are in lineID format, thus use df[1] to get the actual dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yeah, or turn you into toast.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1].loc[df[1].lineID == temp[2], 'dialogue'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text and creating corpus of all sentences and tokenizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"i'm\", \"i am\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"he's\", \"he is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"she's\", \"she is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"it's\", \"it is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"that's\", \"that is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"what's\", \"what is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"where's\", \"where is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"how's\", \"how is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"\\'ll\", \" will\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"\\'ve\", \" have\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"\\'re\", \" are\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"\\'d\", \" would\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"won't\", \"will not\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"can't\", \"cannot\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"n't\", \" not\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"n'\", \"ng\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"'bout\", \"about\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"'til\", \"until\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(\"[\\\"#$%&()*+-/:;<=>@[\\]^_`'{|}~]\", '', sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a vocabulary of all the words used in the corpus and mapping words to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take lineID to dialogue in lists\n",
    "corpus = df[1].dialogue.tolist()\n",
    "corpus = [\"<start> \" + sentence + \" <eos>\" for sentence in corpus]\n",
    "line_id = df[1].lineID.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now take id and dialogue in dictionary\n",
    "\n",
    "dataframe_dict = dict(zip(line_id, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> No! You are not dating until your sister starts dating  End of discussion <eos>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_dict[\"L180\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split X and Y\n",
    "##### Making \"Message to Response\" lists i.e. X and Y (using I)\n",
    "\n",
    "##### Say a row in df[0] is ['L194', 'L195', 'L196', 'L197'], then we want (X[0] = L194, Y[0] = L195), (X[1] = L195, Y[1] = L196), and so on\n",
    "##### We convert LineIDs to dialogues here itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 83097/83097 [00:01<00:00, 61303.85it/s]\n"
     ]
    }
   ],
   "source": [
    "x_encoder = []\n",
    "x_decoder = []\n",
    "y_ = []\n",
    "df_0_len = len(df[0])\n",
    "for i in tqdm(range(df_0_len)):\n",
    "    temp = re.sub(\"[\\\"\\'[\\]\\\\s+]\", \"\", df[0].iloc[i]).split(\",\")\n",
    "    length = len(temp) - 1\n",
    "    for j in range(length):\n",
    "        x = dataframe_dict[temp[j]]\n",
    "        y = dataframe_dict[temp[j + 1]]\n",
    "        \n",
    "        x_encoder.append(x[8: len(x) - 6])\n",
    "        x_decoder.append(y[:len(y) - 6])\n",
    "        y_.append(y[8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Jesus Christ look at all the dust on my carwhy in the hell do not he take it to a car wash?',\n",
       " '<start> Did not know you darker people went in for foreign jobs',\n",
       " 'Did not know you darker people went in for foreign jobs <eos>')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_encoder[1500], x_decoder[1500], y_[1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing sequences into list of integers and padding to maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing to sequences\n",
    "X_encoder = tokenizer.texts_to_sequences(x_encoder)\n",
    "X_decoder = tokenizer.texts_to_sequences(x_decoder)\n",
    "y = tokenizer.texts_to_sequences(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "X_encoder = pad_sequences(X_encoder, maxlen=max_len, dtype='int32', padding='post', truncating='post')\n",
    "X_decoder = pad_sequences(X_decoder, maxlen=max_len, dtype='int32', padding='post', truncating='post')\n",
    "y = pad_sequences(y, maxlen=max_len, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((221616, 20), (221616, 20), (221616, 20))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoder.shape, X_decoder.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(temp) for temp in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping X, Y to pickle for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_and_y.pkl', 'wb') as f:\n",
    "    pickle.dump([X_encoder, X_decoder, y], f, protocol=4)\n",
    "\n",
    "# loading X and Y\n",
    "with open('x_and_y.pkl', 'rb') as f:\n",
    "    X_encoder, X_decoder, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {}\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if idx <= vocab_size:\n",
    "        idx2word[idx] = word\n",
    "    if idx > vocab_size:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "for idx, word in idx2word.items():\n",
    "    word2idx[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx) == len(idx2word), len(word2idx) == 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embeddings pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that returns the word vector for a given word (in string) from the dataframe of word vectors obtained in the next cell\n",
    "def get_vector(word):\n",
    "    return words.loc[word].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a pandas dataframe of entries from the glove pre-trained vectors txt file as running a loop for getting word to vector mapping is expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = pd.read_table(\"glove.840B.300d/glove.840B.300d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since it is computationally expensive to make a dataframe from txt each time, dumping the dataframe to a pickle; load from \"words.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping the dataframe\n",
    "#with open('words.pkl', 'wb') as f:\n",
    "#    pickle.dump(words, f, protocol=4)\n",
    "\n",
    "# loading the dataframe\n",
    "with open('words.pkl', 'rb') as f:\n",
    "    words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocab_embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15000/15000 [07:56<00:00, 31.49it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_embedding = np.zeros((len(word2idx), 300))     # embedding dim is 300\n",
    "\n",
    "for i in tqdm(idx2word.keys()):\n",
    "    try:\n",
    "        temp = get_vector(idx2word[i])\n",
    "        vocab_embedding[i] = temp\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump vocab_embedding to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_embedding, f, protocol=4)\n",
    "\n",
    "# loading vocab_embeddings\n",
    "with open('embedding_weights.pkl', 'rb') as f:\n",
    "    vocab_embedding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
