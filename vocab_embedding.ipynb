{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "##### Dataset: Cornell Movie Dialogues\n",
    "- movie_conversations.txt (the structure of the conversations)\n",
    "\t- fields\n",
    "\t\t- characterID of the first character involved in the conversation\n",
    "\t\t- characterID of the second character involved in the conversation\n",
    "\t\t- movieID of the movie in which the conversation occurred\n",
    "\t\t- list of the utterances that make the conversation, in chronological \n",
    "\t\t\torder: ['lineID1','lineID2',...,'lineIDN']\n",
    "\t\t\thas to be matched with movie_lines.txt to reconstruct the actual content\n",
    "            \n",
    "            \n",
    "- movie_lines.txt (contains the actual text of each utterance)\n",
    "\t- fields:\n",
    "\t\t- lineID\n",
    "\t\t- characterID (who uttered this phrase)\n",
    "\t\t- movieID\n",
    "\t\t- character name\n",
    "\t\t- text of the utterance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"cornell_movie-dialogs_corpus/movie_conversations.txt\", \"cornell_movie-dialogs_corpus/movie_lines.txt\"]\n",
    "df = []\n",
    "for path in paths:\n",
    "    file = open(path)\n",
    "    lines = []\n",
    "    for line in file.readlines():\n",
    "        lines.append(line.replace('\\n', \"\").split(\" +++$+++ \"))\n",
    "    df.append(pd.DataFrame(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df[0] is movie conversations (\"Dataframe of\" \"list of\" dialogues (lineID) in a conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['L194', 'L195', 'L196', 'L197']\n",
       "1                    ['L198', 'L199']\n",
       "2    ['L200', 'L201', 'L202', 'L203']\n",
       "3            ['L204', 'L205', 'L206']\n",
       "4                    ['L207', 'L208']\n",
       "Name: dialogues_list, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning the column titles to the df\n",
    "df[0].columns = ['characterID_1', 'characterID_2', 'movieID', 'dialogues_list']\n",
    "\n",
    "# reducing the dataframe to required fields\n",
    "df[0] = df[0]['dialogues_list']\n",
    "df[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df[1] is movie lines (Dataframe of line ID and Dialogue text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lineID</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lineID      dialogue\n",
       "0  L1045  They do not!\n",
       "1  L1044   They do to!\n",
       "2   L985    I hope so.\n",
       "3   L984     She okay?\n",
       "4   L925     Let's go."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning the column titles to the df\n",
    "df[1].columns = ['lineID', 'characterID', 'movieID', 'character_name', 'dialogue']\n",
    "\n",
    "# reducing the dataframe to required fields\n",
    "df[1] = df[1][['lineID', 'dialogue']]\n",
    "df[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value of df[0] is in the form of list, but it returns a string, so replace characters ', [, ], space with empty string and make a list of 'lineIDs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L8200', 'L8201', 'L8202']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df[0].iloc[1000].replace(\"\\\"\", \"\").replace(\"\\'\", \"\").replace('[', \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of above notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = []\n",
    "for row in df[0]:\n",
    "    conversations.append(row.replace(\"\\\"\", \"\").replace(\"\\'\", \"\").replace('[', \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\"))\n",
    "\n",
    "conversations[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "    sentence = re.sub(r\"i’m\", \"i am\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "    sentence = re.sub(r\"he’s\", \"he is\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "    sentence = re.sub(r\"she’s\", \"she is\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "    sentence = re.sub(r\"it’s\", \"it is\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"that’s\", \"that is\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
    "    sentence = re.sub(r\"what’s\", \"what is\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "    sentence = re.sub(r\"where’s\", \"where is\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"there's\", \"there is\", sentence)\n",
    "    sentence = re.sub(r\"there’s\", \"there is\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"who's\", \"who is\", sentence)\n",
    "    sentence = re.sub(r\"who’s\", \"who is\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "    sentence = re.sub(r\"how’s\", \"how is\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"’ll\", \" will\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"’ve\", \" have\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"’re\", \" are\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"’d\", \" would\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"won’t\", \"will not\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "    sentence = re.sub(r\"can’t\", \"cannot\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"n’t\", \" not\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "    sentence = re.sub(r\"n’\", \"ng\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "    sentence = re.sub(r\"’bout\", \"about\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"'til\", \"until\", sentence)\n",
    "    sentence = re.sub(r\"’til\", \"until\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"c'mon\", \"come on\", sentence)\n",
    "    sentence = re.sub(r\"c’mon\", \"come on\", sentence)\n",
    "\n",
    "    sentence = re.sub(\"[-*/()\\\"’'#/@;:<>{}`+=~|.!?,]\", \"\", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Would you mind getting me a drink, Cameron?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1].dialogue[79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: clean_text(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'would you mind getting me a drink cameron'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1].dialogue[79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a dict to map lineIDs to corresponding Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take lineID to dialogue in lists\n",
    "dialogues = df[1].dialogue.tolist()\n",
    "line_id = df[1].lineID.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now take id and dialogue in dictionary\n",
    "id2dialogue = dict(zip(line_id, dialogues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split X and Y\n",
    "##### Making \"Message to Response\" lists i.e. X and Y\n",
    "\n",
    "##### Say a conversation is ['L194', 'L195', 'L196', 'L197'], then we want (X[0] = L194, Y[0] = L195), (X[1] = L195, Y[1] = L196), and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoder = []\n",
    "x_decoder = []\n",
    "y_ = []\n",
    "\n",
    "for conversation in conversations:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        x_encoder.append(id2dialogue[conversation[i]])\n",
    "        x_decoder.append(id2dialogue[conversation[i + 1]])\n",
    "        y_.append(id2dialogue[conversation[i + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jesus christ look at all the dust on my carwhy in the hell do not he take it to a car wash',\n",
       " 'did not know you darker people went in for foreign jobs',\n",
       " 'did not know you darker people went in for foreign jobs')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_encoder[1500], x_decoder[1500], y_[1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We take max_len as 20 since 85% of the sentences have a length approximately close to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the x that are too long\n",
    "x_encoder_temp = []\n",
    "x_decoder_temp = []\n",
    "y_temp = []\n",
    "\n",
    "i = 0\n",
    "for x in x_encoder:\n",
    "    if len(x.split()) <= max_len:\n",
    "        x_encoder_temp.append(x)\n",
    "        x_decoder_temp.append(x_decoder[i])\n",
    "        y_temp.append(y_[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the y that are too long\n",
    "x_encoder = []\n",
    "x_decoder = []\n",
    "y_ = []\n",
    "\n",
    "i = 0\n",
    "for y in y_temp:\n",
    "    if len(y.split()) <= max_len:\n",
    "        x_decoder.append(y)\n",
    "        y_.append(y)\n",
    "        x_encoder.append(x_encoder_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167126, 167126, 167126)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_encoder), len(x_decoder), len(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a vocabulary of all the words used in the corpus and mapping words to indexes\n",
    "#### We take the word frequency for indexing of the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "\n",
    "for x in x_encoder:\n",
    "    for word in x.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for y in y_:\n",
    "    for word in y.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rare words from the vocabulary.\n",
    "#### We will aim to replace fewer than 5% of words with unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5\n",
    "count = 0\n",
    "for word, freq in vocab.items():\n",
    "    if freq >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46200, 14356)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thus we take\n",
    "vocab_size = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing sequences into list of integers and padding to maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we sort the vocab according to the word frequency\n",
    "vocab = dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index vocab based on frequency\n",
    "idx2word = {0: '<pad>', 1: '<eos>', 2: '<unk>', 3: '<go>'}\n",
    "idx = 4\n",
    "for word in vocab.keys():\n",
    "    idx2word[idx] = word\n",
    "    idx += 1\n",
    "    if idx == vocab_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "for idx, word in idx2word.items():\n",
    "    word2idx[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 15000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2word), len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_decoder)):\n",
    "    x_decoder[i] = \"<go> \" + x_decoder[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_)):\n",
    "    y_[i] += \" <eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<go> lay off asshole', 'lay off asshole <eos>')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_decoder[3000], y_[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, word_count, unk_count):\n",
    "    tokens = []\n",
    "    for word in sentence.split():\n",
    "        if word not in word2idx:\n",
    "            tokens.append(word2idx['<unk>'])\n",
    "            unk_count += 1\n",
    "        else:\n",
    "            tokens.append(word2idx[word])\n",
    "            word_count += 1\n",
    "            \n",
    "    return tokens, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing to sequences\n",
    "# the words occuring in sentences that are not present in vocab are replace by unk\n",
    "X_encoder = []\n",
    "X_decoder = []\n",
    "Y = []\n",
    "\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for x in x_encoder:\n",
    "    tokens, word_count, unk_count = tokenize(x, word_count, unk_count)\n",
    "    X_encoder.append(tokens)\n",
    "    \n",
    "for x in x_decoder:\n",
    "    tokens, word_count, unk_count = tokenize(x, word_count, unk_count)    \n",
    "    X_decoder.append(tokens)\n",
    "    \n",
    "for y in y_:\n",
    "    tokens, _, _ = tokenize(y, word_count, unk_count)\n",
    "    Y.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.178284142784869"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of unk words in word_count\n",
    "(unk_count / word_count) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "X_encoder = pad_sequences(X_encoder, maxlen=max_len, dtype='int32', padding='post', truncating='post')\n",
    "X_decoder = pad_sequences(X_decoder, maxlen=max_len, dtype='int32', padding='post', truncating='post')\n",
    "Y = pad_sequences(Y, maxlen=max_len, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((167126, 20), (167126, 20), (167126, 20))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoder.shape, X_decoder.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0]),\n",
       " array([  3,  85,   4,  15, 119, 107,   9, 650,  43,   9, 580,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]),\n",
       " array([ 85,   4,  15, 119, 107,   9, 650,  43,   9, 580,   1,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoder[30], X_decoder[30], Y[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping word2idx and idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_idx.pkl', 'wb') as f:\n",
    "    pickle.dump([word2idx, idx2word], f, protocol=4)\n",
    "\n",
    "# loading X and Y\n",
    "with open('word_idx.pkl', 'rb') as f:\n",
    "    word2idx, idx2word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping X, Y to pickle for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_and_y.pkl', 'wb') as f:\n",
    "    pickle.dump([X_encoder, X_decoder, Y], f, protocol=4)\n",
    "\n",
    "# loading X and Y\n",
    "with open('x_and_y.pkl', 'rb') as f:\n",
    "    X_encoder, X_decoder, Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embeddings pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that returns the word vector for a given word (in string) from the dataframe of word vectors obtained in the next cell\n",
    "def get_vector(word):\n",
    "    return words.loc[word].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a pandas dataframe of entries from the glove pre-trained vectors txt file as running a loop for getting word to vector mapping is expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = pd.read_table(\"glove.840B.300d/glove.840B.300d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since it is computationally expensive to make a dataframe from txt each time, dumping the dataframe to a pickle; load from \"words.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping the dataframe\n",
    "#with open('words.pkl', 'wb') as f:\n",
    "#    pickle.dump(words, f, protocol=4)\n",
    "\n",
    "# loading the dataframe\n",
    "with open('words.pkl', 'rb') as f:\n",
    "    words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocab_embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab_embedding = np.zeros((len(word2idx), 300))     # embedding dim is 300\n",
    "\n",
    "for i in idx2word.keys():\n",
    "    try:\n",
    "        temp = get_vector(idx2word[i])\n",
    "        vocab_embedding[i] = temp\n",
    "    except:\n",
    "        print(idx2word[i] + \" not in glove\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump vocab_embedding to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_embedding, f, protocol=4)\n",
    "\n",
    "# loading vocab_embeddings\n",
    "with open('embedding_weights.pkl', 'rb') as f:\n",
    "    vocab_embedding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
