{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "##### Dataset: Cornell Movie Dialogues\n",
    "- movie_conversations.txt (the structure of the conversations)\n",
    "\t- fields\n",
    "\t\t- characterID of the first character involved in the conversation\n",
    "\t\t- characterID of the second character involved in the conversation\n",
    "\t\t- movieID of the movie in which the conversation occurred\n",
    "\t\t- list of the utterances that make the conversation, in chronological \n",
    "\t\t\torder: ['lineID1','lineID2',...,'lineIDN']\n",
    "\t\t\thas to be matched with movie_lines.txt to reconstruct the actual content\n",
    "            \n",
    "            \n",
    "- movie_lines.txt (contains the actual text of each utterance)\n",
    "\t- fields:\n",
    "\t\t- lineID\n",
    "\t\t- characterID (who uttered this phrase)\n",
    "\t\t- movieID\n",
    "\t\t- character name\n",
    "\t\t- text of the utterance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"cornell_movie-dialogs_corpus/movie_conversations.txt\", \"cornell_movie-dialogs_corpus/movie_lines.txt\"]\n",
    "df = []\n",
    "for path in paths:\n",
    "    file = open(path)\n",
    "    lines = []\n",
    "    for line in file.readlines():\n",
    "        lines.append(line.replace('\\n', \"\").split(\" +++$+++ \"))\n",
    "    df.append(pd.DataFrame(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df[0] is movie conversations (\"Dataframe of\" \"list of\" dialogues (lineID) in a conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['L194', 'L195', 'L196', 'L197']\n",
       "1                    ['L198', 'L199']\n",
       "2    ['L200', 'L201', 'L202', 'L203']\n",
       "3            ['L204', 'L205', 'L206']\n",
       "4                    ['L207', 'L208']\n",
       "Name: dialogues_list, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning the column titles to the df\n",
    "df[0].columns = ['characterID_1', 'characterID_2', 'movieID', 'dialogues_list']\n",
    "\n",
    "# reducing the dataframe to required fields\n",
    "df[0] = df[0]['dialogues_list']\n",
    "df[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df[1] is movie lines (Dataframe of line ID and Dialogue text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lineID</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lineID      dialogue\n",
       "0  L1045  They do not!\n",
       "1  L1044   They do to!\n",
       "2   L985    I hope so.\n",
       "3   L984     She okay?\n",
       "4   L925     Let's go."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning the column titles to the df\n",
    "df[1].columns = ['lineID', 'characterID', 'movieID', 'character_name', 'dialogue']\n",
    "\n",
    "# reducing the dataframe to required fields\n",
    "df[1] = df[1][['lineID', 'dialogue']]\n",
    "df[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value of df[0] is in the form of list, but it returns a string, so replace characters ', [, ], space with empty string and make a list of 'lineIDs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L8200', 'L8201', 'L8202']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df[0].iloc[1000].replace(\"\\\"\", \"\").replace(\"\\'\", \"\").replace('[', \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of above notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = []\n",
    "for row in df[0]:\n",
    "    conversations.append(row.replace(\"\\\"\", \"\").replace(\"\\'\", \"\").replace('[', \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\"))\n",
    "\n",
    "conversations[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: sentences.lower())\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"i'm\", \"i am\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"i’m\", \"i am\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"he's\", \"he is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"he’s\", \"he is\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"she's\", \"she is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"she’s\", \"she is\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"it's\", \"it is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"it’s\", \"it is\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"that's\", \"that is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"that’s\", \"that is\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"what's\", \"what is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"what’s\", \"what is\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"where's\", \"where is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"where’s\", \"where is\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"there's\", \"there is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"there’s\", \"there is\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"who's\", \"who is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"who’s\", \"who is\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"how's\", \"how is\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"how’s\", \"how is\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"\\'ll\", \" will\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"’ll\", \" will\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"\\'ve\", \" have\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"’ve\", \" have\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"\\'re\", \" are\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"’re\", \" are\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"\\'d\", \" would\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"’d\", \" would\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"won't\", \"will not\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"won’t\", \"will not\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"can't\", \"cannot\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"can’t\", \"cannot\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"n't\", \" not\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"n’t\", \" not\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"n'\", \"ng\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"n’\", \"ng\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"'bout\", \"about\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"’bout\", \"about\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"'til\", \"until\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"’til\", \"until\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"c'mon\", \"come on\", sentences))\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(r\"c’mon\", \"come on\", sentences))\n",
    "\n",
    "df[1].dialogue = df[1].dialogue.apply(lambda sentences: re.sub(\"[-*/()\\\"’'#/@;:<>{}`+=~|.!?,]\", \"\", sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a dict to map lineIDs to corresponding Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take lineID to dialogue in lists\n",
    "dialogues = df[1].dialogue.tolist()\n",
    "line_id = df[1].lineID.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now take id and dialogue in dictionary\n",
    "id2dialogue = dict(zip(line_id, dialogues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split X and Y\n",
    "##### Making \"Message to Response\" lists i.e. X and Y\n",
    "\n",
    "##### Say a conversation is ['L194', 'L195', 'L196', 'L197'], then we want (X[0] = L194, Y[0] = L195), (X[1] = L195, Y[1] = L196), and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoder = []\n",
    "x_decoder = []\n",
    "y_ = []\n",
    "\n",
    "for conversation in conversations:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        x_encoder.append(id2dialogue[conversation[i]])\n",
    "        x_decoder.append(id2dialogue[conversation[i + 1]])\n",
    "        y_.append(id2dialogue[conversation[i + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jesus christ look at all the dust on my carwhy in the hell do not he take it to a car wash',\n",
       " 'did not know you darker people went in for foreign jobs',\n",
       " 'did not know you darker people went in for foreign jobs')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_encoder[1500], x_decoder[1500], y_[1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We take max_len as 20 since 85% of the sentences have a length approximately close to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the x that are too long\n",
    "x_encoder_temp = []\n",
    "x_decoder_temp = []\n",
    "y_temp = []\n",
    "\n",
    "i = 0\n",
    "for x in x_encoder:\n",
    "    if len(x.split()) <= max_len:\n",
    "        x_encoder_temp.append(x)\n",
    "        x_decoder_temp.append(x_decoder[i])\n",
    "        y_temp.append(y_[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the y that are too long\n",
    "x_encoder = []\n",
    "x_decoder = []\n",
    "y_ = []\n",
    "\n",
    "i = 0\n",
    "for y in y_temp:\n",
    "    if len(y.split()) <= max_len:\n",
    "        x_decoder.append(y)\n",
    "        y_.append(y)\n",
    "        x_encoder.append(x_encoder_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167126, 167126, 167126)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_encoder), len(x_decoder), len(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a vocabulary of all the words used in the corpus and mapping words to indexes\n",
    "#### We take the word frequency for indexing of the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "\n",
    "for x in x_encoder:\n",
    "    for word in x.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for y in y_:\n",
    "    for word in y.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rare words from the vocabulary.\n",
    "#### We will aim to replace fewer than 5% of words with unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5\n",
    "count = 0\n",
    "for word, freq in vocab.items():\n",
    "    if freq >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46200, 14356)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thus we take\n",
    "vocab_size = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing sequences into list of integers and padding to maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we sort the vocab according to the word frequency\n",
    "vocab = dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index vocab based on frequency\n",
    "idx2word = {0: '<pad>', 1: '<eos>', 2: '<unk>', 3: '<go>'}\n",
    "idx = 4\n",
    "for word in vocab.keys():\n",
    "    idx2word[idx] = word\n",
    "    idx += 1\n",
    "    if idx == vocab_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "for idx, word in idx2word.items():\n",
    "    word2idx[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 15000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2word), len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: '<eos>',\n",
       " 2: '<unk>',\n",
       " 3: '<go>',\n",
       " 4: 'you',\n",
       " 5: 'i',\n",
       " 6: 'is',\n",
       " 7: 'the',\n",
       " 8: 'not',\n",
       " 9: 'to',\n",
       " 10: 'it',\n",
       " 11: 'a',\n",
       " 12: 'do',\n",
       " 13: 'what',\n",
       " 14: 'that',\n",
       " 15: 'are',\n",
       " 16: 'have',\n",
       " 17: 'me',\n",
       " 18: 'of',\n",
       " 19: 'and',\n",
       " 20: 'in',\n",
       " 21: 'we',\n",
       " 22: 'am',\n",
       " 23: 'he',\n",
       " 24: 'no',\n",
       " 25: 'will',\n",
       " 26: 'this',\n",
       " 27: 'know',\n",
       " 28: 'for',\n",
       " 29: 'your',\n",
       " 30: 'was',\n",
       " 31: 'my',\n",
       " 32: 'on',\n",
       " 33: 'be',\n",
       " 34: 'did',\n",
       " 35: 'just',\n",
       " 36: 'would',\n",
       " 37: 'about',\n",
       " 38: 'they',\n",
       " 39: 'with',\n",
       " 40: 'like',\n",
       " 41: 'but',\n",
       " 42: 'get',\n",
       " 43: 'how',\n",
       " 44: 'there',\n",
       " 45: 'all',\n",
       " 46: 'here',\n",
       " 47: 'so',\n",
       " 48: 'she',\n",
       " 49: 'want',\n",
       " 50: 'yes',\n",
       " 51: 'out',\n",
       " 52: 'him',\n",
       " 53: 'got',\n",
       " 54: 'think',\n",
       " 55: 'well',\n",
       " 56: 'can',\n",
       " 57: 'why',\n",
       " 58: 'right',\n",
       " 59: 'yeah',\n",
       " 60: 'up',\n",
       " 61: 'go',\n",
       " 62: 'if',\n",
       " 63: 'oh',\n",
       " 64: 'going',\n",
       " 65: 'at',\n",
       " 66: 'one',\n",
       " 67: 'now',\n",
       " 68: 'where',\n",
       " 69: 'who',\n",
       " 70: 'her',\n",
       " 71: 'see',\n",
       " 72: 'good',\n",
       " 73: 'come',\n",
       " 74: 'cannot',\n",
       " 75: 'tell',\n",
       " 76: 'say',\n",
       " 77: 'were',\n",
       " 78: 'could',\n",
       " 79: 'from',\n",
       " 80: 'been',\n",
       " 81: 'time',\n",
       " 82: 'an',\n",
       " 83: 'back',\n",
       " 84: 'some',\n",
       " 85: 'okay',\n",
       " 86: 'look',\n",
       " 87: 'something',\n",
       " 88: 'when',\n",
       " 89: 'then',\n",
       " 90: 'as',\n",
       " 91: 'mean',\n",
       " 92: 'his',\n",
       " 93: 'take',\n",
       " 94: 'them',\n",
       " 95: 'sure',\n",
       " 96: 'us',\n",
       " 97: 'never',\n",
       " 98: 'does',\n",
       " 99: 'or',\n",
       " 100: 'too',\n",
       " 101: 'man',\n",
       " 102: 'really',\n",
       " 103: 'had',\n",
       " 104: 'should',\n",
       " 105: 'way',\n",
       " 106: 'said',\n",
       " 107: 'need',\n",
       " 108: 'maybe',\n",
       " 109: 'down',\n",
       " 110: 'make',\n",
       " 111: 'any',\n",
       " 112: 'little',\n",
       " 113: 'doing',\n",
       " 114: 'sorry',\n",
       " 115: 'mr',\n",
       " 116: 'sir',\n",
       " 117: 'more',\n",
       " 118: 'very',\n",
       " 119: 'gonna',\n",
       " 120: 'nothing',\n",
       " 121: 'much',\n",
       " 122: 'talk',\n",
       " 123: 'over',\n",
       " 124: 'off',\n",
       " 125: 'let',\n",
       " 126: 'anything',\n",
       " 127: 'thought',\n",
       " 128: 'thing',\n",
       " 129: 'only',\n",
       " 130: 'has',\n",
       " 131: 'two',\n",
       " 132: 'call',\n",
       " 133: 'love',\n",
       " 134: 'give',\n",
       " 135: 'our',\n",
       " 136: 'hey',\n",
       " 137: 'by',\n",
       " 138: 'people',\n",
       " 139: 'please',\n",
       " 140: 'must',\n",
       " 141: 'still',\n",
       " 142: 'told',\n",
       " 143: 'better',\n",
       " 144: 'lets',\n",
       " 145: 'work',\n",
       " 146: 'ever',\n",
       " 147: 'night',\n",
       " 148: 'long',\n",
       " 149: 'help',\n",
       " 150: 'because',\n",
       " 151: 'before',\n",
       " 152: 'find',\n",
       " 153: 'thank',\n",
       " 154: 'name',\n",
       " 155: 'god',\n",
       " 156: 'last',\n",
       " 157: 'believe',\n",
       " 158: 'talking',\n",
       " 159: 'money',\n",
       " 160: 'shit',\n",
       " 161: 'again',\n",
       " 162: 'around',\n",
       " 163: 'fucking',\n",
       " 164: 'course',\n",
       " 165: 'always',\n",
       " 166: 'things',\n",
       " 167: 'hell',\n",
       " 168: 'even',\n",
       " 169: 'first',\n",
       " 170: 'fine',\n",
       " 171: 'fuck',\n",
       " 172: 'other',\n",
       " 173: 'great',\n",
       " 174: 'those',\n",
       " 175: 'these',\n",
       " 176: 'home',\n",
       " 177: 'huh',\n",
       " 178: 'wait',\n",
       " 179: 'guess',\n",
       " 180: 'away',\n",
       " 181: 'put',\n",
       " 182: 'than',\n",
       " 183: 'guy',\n",
       " 184: 'into',\n",
       " 185: 'old',\n",
       " 186: 'keep',\n",
       " 187: 'thanks',\n",
       " 188: 'remember',\n",
       " 189: 'bad',\n",
       " 190: 'wrong',\n",
       " 191: 'dead',\n",
       " 192: 'leave',\n",
       " 193: 'after',\n",
       " 194: 'feel',\n",
       " 195: 'uh',\n",
       " 196: 'new',\n",
       " 197: 'happened',\n",
       " 198: 'kind',\n",
       " 199: 'ask',\n",
       " 200: 'place',\n",
       " 201: 'kill',\n",
       " 202: 'hear',\n",
       " 203: 'life',\n",
       " 204: 'nice',\n",
       " 205: 'big',\n",
       " 206: 'day',\n",
       " 207: 'stop',\n",
       " 208: 'else',\n",
       " 209: 'years',\n",
       " 210: 'ai',\n",
       " 211: 'girl',\n",
       " 212: 'mind',\n",
       " 213: 'everything',\n",
       " 214: 'getting',\n",
       " 215: 'gotta',\n",
       " 216: 'understand',\n",
       " 217: 'stay',\n",
       " 218: 'three',\n",
       " 219: 'coming',\n",
       " 220: 'enough',\n",
       " 221: 'lot',\n",
       " 222: 'might',\n",
       " 223: 'real',\n",
       " 224: 'car',\n",
       " 225: 'looking',\n",
       " 226: 'wanted',\n",
       " 227: 'made',\n",
       " 228: 'another',\n",
       " 229: 'yourself',\n",
       " 230: 'tonight',\n",
       " 231: 'done',\n",
       " 232: 'try',\n",
       " 233: 'heard',\n",
       " 234: 'saw',\n",
       " 235: 'listen',\n",
       " 236: 'friend',\n",
       " 237: 'father',\n",
       " 238: 'house',\n",
       " 239: 'someone',\n",
       " 240: 'left',\n",
       " 241: 'seen',\n",
       " 242: 'miss',\n",
       " 243: 'ya',\n",
       " 244: 'guys',\n",
       " 245: 'hello',\n",
       " 246: 'trying',\n",
       " 247: 'their',\n",
       " 248: 'pretty',\n",
       " 249: 'mother',\n",
       " 250: 'care',\n",
       " 251: 'came',\n",
       " 252: 'hi',\n",
       " 253: 'boy',\n",
       " 254: 'room',\n",
       " 255: 'may',\n",
       " 256: 'morning',\n",
       " 257: 'knew',\n",
       " 258: 'through',\n",
       " 259: 'being',\n",
       " 260: 'tomorrow',\n",
       " 261: 'killed',\n",
       " 262: 'job',\n",
       " 263: 'went',\n",
       " 264: 'idea',\n",
       " 265: 'yet',\n",
       " 266: 'own',\n",
       " 267: 'matter',\n",
       " 268: 'same',\n",
       " 269: 'baby',\n",
       " 270: 'business',\n",
       " 271: 'many',\n",
       " 272: 'show',\n",
       " 273: 'live',\n",
       " 274: 'em',\n",
       " 275: 'dad',\n",
       " 276: 'every',\n",
       " 277: 'best',\n",
       " 278: 'called',\n",
       " 279: 'minute',\n",
       " 280: 'already',\n",
       " 281: 'five',\n",
       " 282: 'use',\n",
       " 283: 'play',\n",
       " 284: 'meet',\n",
       " 285: 'today',\n",
       " 286: 'jesus',\n",
       " 287: 'saying',\n",
       " 288: 'alone',\n",
       " 289: 'found',\n",
       " 290: 'next',\n",
       " 291: 'shut',\n",
       " 292: 'mrs',\n",
       " 293: 'looks',\n",
       " 294: 'alright',\n",
       " 295: 'wanna',\n",
       " 296: 'probably',\n",
       " 297: 'used',\n",
       " 298: 'says',\n",
       " 299: 'crazy',\n",
       " 300: 'wife',\n",
       " 301: 'exactly',\n",
       " 302: 'stuff',\n",
       " 303: 'problem',\n",
       " 304: 'which',\n",
       " 305: 'forget',\n",
       " 306: 'friends',\n",
       " 307: 'together',\n",
       " 308: 'run',\n",
       " 309: 'afraid',\n",
       " 310: 'hundred',\n",
       " 311: 'damn',\n",
       " 312: 'school',\n",
       " 313: 'late',\n",
       " 314: 'start',\n",
       " 315: 'worry',\n",
       " 316: 'four',\n",
       " 317: 'gone',\n",
       " 318: 'few',\n",
       " 319: 'hope',\n",
       " 320: 'without',\n",
       " 321: 'myself',\n",
       " 322: 'whole',\n",
       " 323: 'ago',\n",
       " 324: 'drink',\n",
       " 325: 'son',\n",
       " 326: 'doctor',\n",
       " 327: 'mom',\n",
       " 328: 'minutes',\n",
       " 329: 'woman',\n",
       " 330: 'ten',\n",
       " 331: 'working',\n",
       " 332: 'world',\n",
       " 333: 'took',\n",
       " 334: 'hold',\n",
       " 335: 'wants',\n",
       " 336: 'while',\n",
       " 337: 'supposed',\n",
       " 338: 'somebody',\n",
       " 339: 'days',\n",
       " 340: 'ready',\n",
       " 341: 'watch',\n",
       " 342: 'read',\n",
       " 343: 'once',\n",
       " 344: 'hard',\n",
       " 345: 'word',\n",
       " 346: 'hurt',\n",
       " 347: 'anyway',\n",
       " 348: 'bring',\n",
       " 349: 'since',\n",
       " 350: 'until',\n",
       " 351: 'head',\n",
       " 352: 'open',\n",
       " 353: 'sleep',\n",
       " 354: 'anyone',\n",
       " 355: 'actually',\n",
       " 356: 'men',\n",
       " 357: 'die',\n",
       " 358: 'true',\n",
       " 359: 'jack',\n",
       " 360: 'beautiful',\n",
       " 361: 'kid',\n",
       " 362: 'thinking',\n",
       " 363: 'married',\n",
       " 364: 'sit',\n",
       " 365: 'john',\n",
       " 366: 'such',\n",
       " 367: 'dr',\n",
       " 368: 'honey',\n",
       " 369: 'most',\n",
       " 370: 'question',\n",
       " 371: 'knows',\n",
       " 372: 'deal',\n",
       " 373: 'ok',\n",
       " 374: 'move',\n",
       " 375: 'easy',\n",
       " 376: 'point',\n",
       " 377: 'taking',\n",
       " 378: 'lost',\n",
       " 379: 'kids',\n",
       " 380: 'makes',\n",
       " 381: 'week',\n",
       " 382: 'later',\n",
       " 383: 'turn',\n",
       " 384: 'asked',\n",
       " 385: 'happen',\n",
       " 386: 'phone',\n",
       " 387: 'nobody',\n",
       " 388: 'eat',\n",
       " 389: 'hate',\n",
       " 390: 'case',\n",
       " 391: 'having',\n",
       " 392: 'funny',\n",
       " 393: 'soon',\n",
       " 394: 'happy',\n",
       " 395: 'check',\n",
       " 396: 'wish',\n",
       " 397: 'captain',\n",
       " 398: 'brother',\n",
       " 399: 'excuse',\n",
       " 400: 'story',\n",
       " 401: 'door',\n",
       " 402: 'mine',\n",
       " 403: 'under',\n",
       " 404: 'quite',\n",
       " 405: 'bet',\n",
       " 406: 'far',\n",
       " 407: 'part',\n",
       " 408: 'trust',\n",
       " 409: 'cut',\n",
       " 410: 'yours',\n",
       " 411: 'ass',\n",
       " 412: 'both',\n",
       " 413: 'ah',\n",
       " 414: 'suppose',\n",
       " 415: 'thousand',\n",
       " 416: 'cause',\n",
       " 417: 'second',\n",
       " 418: 'telling',\n",
       " 419: 'six',\n",
       " 420: 'anybody',\n",
       " 421: 'making',\n",
       " 422: 'answer',\n",
       " 423: 'whatever',\n",
       " 424: 'pay',\n",
       " 425: 'dollars',\n",
       " 426: 'trouble',\n",
       " 427: 'hit',\n",
       " 428: 'half',\n",
       " 429: 'least',\n",
       " 430: 'shot',\n",
       " 431: 'gun',\n",
       " 432: 'gave',\n",
       " 433: 'family',\n",
       " 434: 'police',\n",
       " 435: 'change',\n",
       " 436: 'times',\n",
       " 437: 'anymore',\n",
       " 438: 'year',\n",
       " 439: 'different',\n",
       " 440: 'party',\n",
       " 441: 'walk',\n",
       " 442: 'everybody',\n",
       " 443: 'close',\n",
       " 444: 'met',\n",
       " 445: 'waiting',\n",
       " 446: 'chance',\n",
       " 447: 'young',\n",
       " 448: 'face',\n",
       " 449: 'sometimes',\n",
       " 450: 'bit',\n",
       " 451: 'drive',\n",
       " 452: 'important',\n",
       " 453: 'truth',\n",
       " 454: 'kidding',\n",
       " 455: 'town',\n",
       " 456: 'rest',\n",
       " 457: 'end',\n",
       " 458: 'almost',\n",
       " 459: 'bed',\n",
       " 460: 'inside',\n",
       " 461: 'sick',\n",
       " 462: 'number',\n",
       " 463: 'hours',\n",
       " 464: 'serious',\n",
       " 465: 'couple',\n",
       " 466: 'either',\n",
       " 467: 'set',\n",
       " 468: 'hand',\n",
       " 469: 'game',\n",
       " 470: 'tried',\n",
       " 471: 'death',\n",
       " 472: 'office',\n",
       " 473: 'speak',\n",
       " 474: 'christ',\n",
       " 475: 'everyone',\n",
       " 476: 'stupid',\n",
       " 477: 'water',\n",
       " 478: 'gets',\n",
       " 479: 'feeling',\n",
       " 480: 'died',\n",
       " 481: 'each',\n",
       " 482: 'scared',\n",
       " 483: 'george',\n",
       " 484: 'send',\n",
       " 485: 'dear',\n",
       " 486: 'means',\n",
       " 487: 'pick',\n",
       " 488: 'though',\n",
       " 489: 'buy',\n",
       " 490: 'running',\n",
       " 491: 'asking',\n",
       " 492: 'promise',\n",
       " 493: 'book',\n",
       " 494: 'ahead',\n",
       " 495: 'eyes',\n",
       " 496: 'dinner',\n",
       " 497: 'sounds',\n",
       " 498: 'frank',\n",
       " 499: 'stand',\n",
       " 500: 'twenty',\n",
       " 501: 'daddy',\n",
       " 502: 'shoot',\n",
       " 503: 'sort',\n",
       " 504: 'person',\n",
       " 505: 'heart',\n",
       " 506: 'alive',\n",
       " 507: 'break',\n",
       " 508: 'hour',\n",
       " 509: 'side',\n",
       " 510: 'glad',\n",
       " 511: 'ones',\n",
       " 512: 'hot',\n",
       " 513: 'lady',\n",
       " 514: 'perhaps',\n",
       " 515: 'girls',\n",
       " 516: 'women',\n",
       " 517: 'harry',\n",
       " 518: 'leaving',\n",
       " 519: 'war',\n",
       " 520: 'reason',\n",
       " 521: 'behind',\n",
       " 522: 'outside',\n",
       " 523: 'cool',\n",
       " 524: 'comes',\n",
       " 525: 'along',\n",
       " 526: 'white',\n",
       " 527: 'seem',\n",
       " 528: 'goes',\n",
       " 529: 'fire',\n",
       " 530: 'power',\n",
       " 531: 'bullshit',\n",
       " 532: 'calling',\n",
       " 533: 'sound',\n",
       " 534: 'safe',\n",
       " 535: 'shall',\n",
       " 536: 'husband',\n",
       " 537: 'high',\n",
       " 538: 'goddamn',\n",
       " 539: 'boys',\n",
       " 540: 'fun',\n",
       " 541: 'hands',\n",
       " 542: 'goodbye',\n",
       " 543: 'blood',\n",
       " 544: 'black',\n",
       " 545: 'dog',\n",
       " 546: 'sex',\n",
       " 547: 'mister',\n",
       " 548: 'sent',\n",
       " 549: 'line',\n",
       " 550: 'news',\n",
       " 551: 'light',\n",
       " 552: 'wonderful',\n",
       " 553: 'coffee',\n",
       " 554: 'lose',\n",
       " 555: 'months',\n",
       " 556: 'eight',\n",
       " 557: 'uhhuh',\n",
       " 558: 'save',\n",
       " 559: 'body',\n",
       " 560: 'tired',\n",
       " 561: 'write',\n",
       " 562: 'mary',\n",
       " 563: 'needs',\n",
       " 564: 'movie',\n",
       " 565: 'against',\n",
       " 566: 'weeks',\n",
       " 567: 'luck',\n",
       " 568: 'brought',\n",
       " 569: 'bill',\n",
       " 570: 'full',\n",
       " 571: 'living',\n",
       " 572: 'certainly',\n",
       " 573: 'seems',\n",
       " 574: 'happens',\n",
       " 575: 'ship',\n",
       " 576: 'questions',\n",
       " 577: 'million',\n",
       " 578: 'lucky',\n",
       " 579: 'playing',\n",
       " 580: 'lie',\n",
       " 581: 'free',\n",
       " 582: 'fight',\n",
       " 583: 'sister',\n",
       " 584: 'street',\n",
       " 585: 'somewhere',\n",
       " 586: 'special',\n",
       " 587: 'fast',\n",
       " 588: 'figure',\n",
       " 589: 'cold',\n",
       " 590: 'fifty',\n",
       " 591: 'hair',\n",
       " 592: 'front',\n",
       " 593: 'started',\n",
       " 594: 'expect',\n",
       " 595: 'moment',\n",
       " 596: 'president',\n",
       " 597: 'touch',\n",
       " 598: 'sam',\n",
       " 599: 'absolutely',\n",
       " 600: 'pull',\n",
       " 601: 'tom',\n",
       " 602: 'also',\n",
       " 603: 'rather',\n",
       " 604: 'till',\n",
       " 605: 'accident',\n",
       " 606: 'names',\n",
       " 607: 'york',\n",
       " 608: 'follow',\n",
       " 609: 'ride',\n",
       " 610: 'city',\n",
       " 611: 'sense',\n",
       " 612: 'date',\n",
       " 613: 'poor',\n",
       " 614: 'darling',\n",
       " 615: 'between',\n",
       " 616: 'hang',\n",
       " 617: 'none',\n",
       " 618: 'david',\n",
       " 619: 'thinks',\n",
       " 620: 'daughter',\n",
       " 621: 'plan',\n",
       " 622: 'eh',\n",
       " 623: 'picture',\n",
       " 624: 'seven',\n",
       " 625: 'hurry',\n",
       " 626: 'dream',\n",
       " 627: 'mouth',\n",
       " 628: 'outta',\n",
       " 629: 'worth',\n",
       " 630: 'bitch',\n",
       " 631: 'parents',\n",
       " 632: 'food',\n",
       " 633: 'quit',\n",
       " 634: 'words',\n",
       " 635: 'children',\n",
       " 636: 'worse',\n",
       " 637: 'possible',\n",
       " 638: 'worried',\n",
       " 639: 'fact',\n",
       " 640: 'charlie',\n",
       " 641: 'bob',\n",
       " 642: 'control',\n",
       " 643: 'hotel',\n",
       " 644: 'heres',\n",
       " 645: 'looked',\n",
       " 646: 'busy',\n",
       " 647: 'catch',\n",
       " 648: 'himself',\n",
       " 649: 'walter',\n",
       " 650: 'learn',\n",
       " 651: 'drop',\n",
       " 652: 'country',\n",
       " 653: 'maam',\n",
       " 654: 'air',\n",
       " 655: 'meeting',\n",
       " 656: 'miles',\n",
       " 657: 'boss',\n",
       " 658: 'nick',\n",
       " 659: 'sweet',\n",
       " 660: 'interested',\n",
       " 661: 'paul',\n",
       " 662: 'thirty',\n",
       " 663: 'its',\n",
       " 664: 'works',\n",
       " 665: 'wonder',\n",
       " 666: 'clear',\n",
       " 667: 'michael',\n",
       " 668: 'perfect',\n",
       " 669: 'nine',\n",
       " 670: 'finish',\n",
       " 671: 'taken',\n",
       " 672: 'worked',\n",
       " 673: 'company',\n",
       " 674: 'child',\n",
       " 675: 'talked',\n",
       " 676: 'explain',\n",
       " 677: 'anywhere',\n",
       " 678: 'lives',\n",
       " 679: 'swear',\n",
       " 680: 'hospital',\n",
       " 681: 'small',\n",
       " 682: 'others',\n",
       " 683: 'choice',\n",
       " 684: 'lying',\n",
       " 685: 'buddy',\n",
       " 686: 'moving',\n",
       " 687: 'straight',\n",
       " 688: 'major',\n",
       " 689: 'jimmy',\n",
       " 690: 'lunch',\n",
       " 691: 'piece',\n",
       " 692: 'strange',\n",
       " 693: 'dude',\n",
       " 694: 'general',\n",
       " 695: 'king',\n",
       " 696: 'wear',\n",
       " 697: 'seeing',\n",
       " 698: 'throw',\n",
       " 699: 'able',\n",
       " 700: 'marry',\n",
       " 701: 'early',\n",
       " 702: 'careful',\n",
       " 703: 'act',\n",
       " 704: 'known',\n",
       " 705: 'happening',\n",
       " 706: 'ben',\n",
       " 707: 'order',\n",
       " 708: 'report',\n",
       " 709: 'red',\n",
       " 710: 'clothes',\n",
       " 711: 'joe',\n",
       " 712: 'handle',\n",
       " 713: 'kiss',\n",
       " 714: 'evening',\n",
       " 715: 'quiet',\n",
       " 716: 'yesterday',\n",
       " 717: 'liked',\n",
       " 718: 'terrible',\n",
       " 719: 'eddie',\n",
       " 720: 'takes',\n",
       " 721: 'music',\n",
       " 722: 'interesting',\n",
       " 723: 'watching',\n",
       " 724: 'dark',\n",
       " 725: 'sign',\n",
       " 726: 'relax',\n",
       " 727: 'clean',\n",
       " 728: 'less',\n",
       " 729: 'feet',\n",
       " 730: 'peter',\n",
       " 731: 'wow',\n",
       " 732: 'hungry',\n",
       " 733: 'fifteen',\n",
       " 734: 'bucks',\n",
       " 735: 'dance',\n",
       " 736: 'road',\n",
       " 737: 'mistake',\n",
       " 738: 'apartment',\n",
       " 739: 'mad',\n",
       " 740: 'tv',\n",
       " 741: 'ice',\n",
       " 742: 'broke',\n",
       " 743: 'ray',\n",
       " 744: 'unless',\n",
       " 745: 'blow',\n",
       " 746: 'tough',\n",
       " 747: 'kinda',\n",
       " 748: 'dick',\n",
       " 749: 'fault',\n",
       " 750: 'plane',\n",
       " 751: 'nervous',\n",
       " 752: 'cops',\n",
       " 753: 'um',\n",
       " 754: 'state',\n",
       " 755: 'paid',\n",
       " 756: 'missed',\n",
       " 757: 'class',\n",
       " 758: 'james',\n",
       " 759: 'trip',\n",
       " 760: 'private',\n",
       " 761: 'welcome',\n",
       " 762: 'lord',\n",
       " 763: 'fall',\n",
       " 764: 'giving',\n",
       " 765: 'difference',\n",
       " 766: 'jim',\n",
       " 767: 'count',\n",
       " 768: 'chief',\n",
       " 769: 'except',\n",
       " 770: 'top',\n",
       " 771: 'meant',\n",
       " 772: 'smart',\n",
       " 773: 'staying',\n",
       " 774: 'beat',\n",
       " 775: 'forgot',\n",
       " 776: 'fucked',\n",
       " 777: 'bother',\n",
       " 778: 'max',\n",
       " 779: 'asshole',\n",
       " 780: 'past',\n",
       " 781: 'human',\n",
       " 782: 'ought',\n",
       " 783: 'felt',\n",
       " 784: 'officer',\n",
       " 785: 'cop',\n",
       " 786: 'eye',\n",
       " 787: 'blue',\n",
       " 788: 'changed',\n",
       " 789: 'weird',\n",
       " 790: 'store',\n",
       " 791: 'likes',\n",
       " 792: 'fair',\n",
       " 793: 'ha',\n",
       " 794: 'bye',\n",
       " 795: 'upset',\n",
       " 796: 'favor',\n",
       " 797: 'oclock',\n",
       " 798: 'sake',\n",
       " 799: 'boat',\n",
       " 800: 'besides',\n",
       " 801: 'win',\n",
       " 802: 'honest',\n",
       " 803: 'earth',\n",
       " 804: 'longer',\n",
       " 805: 'loved',\n",
       " 806: 'personal',\n",
       " 807: 'twelve',\n",
       " 808: 'christmas',\n",
       " 809: 'building',\n",
       " 810: 'finished',\n",
       " 811: 'near',\n",
       " 812: 'wake',\n",
       " 813: 'calls',\n",
       " 814: 'neither',\n",
       " 815: 'information',\n",
       " 816: 'pardon',\n",
       " 817: 'drunk',\n",
       " 818: 'beer',\n",
       " 819: 'message',\n",
       " 820: 'owe',\n",
       " 821: 'calm',\n",
       " 822: 'rich',\n",
       " 823: 'yah',\n",
       " 824: 'pictures',\n",
       " 825: 'law',\n",
       " 826: 'key',\n",
       " 827: 'month',\n",
       " 828: 'american',\n",
       " 829: 'surprise',\n",
       " 830: 'wearing',\n",
       " 831: 'johnny',\n",
       " 832: 'fool',\n",
       " 833: 'afternoon',\n",
       " 834: 'simple',\n",
       " 835: 'agent',\n",
       " 836: 'turned',\n",
       " 837: 'figured',\n",
       " 838: 'uncle',\n",
       " 839: 'english',\n",
       " 840: 'truck',\n",
       " 841: 'dangerous',\n",
       " 842: 'bastard',\n",
       " 843: 'rose',\n",
       " 844: 'ring',\n",
       " 845: 'secret',\n",
       " 846: 'bank',\n",
       " 847: 'jake',\n",
       " 848: 'slow',\n",
       " 849: 'books',\n",
       " 850: 'lots',\n",
       " 851: 'asleep',\n",
       " 852: 'sitting',\n",
       " 853: 'missing',\n",
       " 854: 'listening',\n",
       " 855: 'lawyer',\n",
       " 856: 'nope',\n",
       " 857: 'bag',\n",
       " 858: 'mama',\n",
       " 859: 'suit',\n",
       " 860: 'doubt',\n",
       " 861: 'dress',\n",
       " 862: 'cash',\n",
       " 863: 'club',\n",
       " 864: 'caught',\n",
       " 865: 'tape',\n",
       " 866: 'ang',\n",
       " 867: 'pleasure',\n",
       " 868: 'driving',\n",
       " 869: 'picked',\n",
       " 870: 'ma',\n",
       " 871: 'seconds',\n",
       " 872: 'voice',\n",
       " 873: 'sell',\n",
       " 874: 'la',\n",
       " 875: 'problems',\n",
       " 876: 'dying',\n",
       " 877: 'list',\n",
       " 878: 'ii',\n",
       " 879: 'bought',\n",
       " 880: 'murder',\n",
       " 881: 'quick',\n",
       " 882: 'table',\n",
       " 883: 'record',\n",
       " 884: 'completely',\n",
       " 885: 'impossible',\n",
       " 886: 'window',\n",
       " 887: 'floor',\n",
       " 888: 'card',\n",
       " 889: 'system',\n",
       " 890: 'lieutenant',\n",
       " 891: 'stick',\n",
       " 892: 'totally',\n",
       " 893: 'eve',\n",
       " 894: 'killing',\n",
       " 895: 'radio',\n",
       " 896: 'wrote',\n",
       " 897: 'fix',\n",
       " 898: 'honor',\n",
       " 899: 'fish',\n",
       " 900: 'gods',\n",
       " 901: 'letter',\n",
       " 902: 'aye',\n",
       " 903: 'gimme',\n",
       " 904: 'needed',\n",
       " 905: 'named',\n",
       " 906: 'army',\n",
       " 907: 'goodnight',\n",
       " 908: 'appreciate',\n",
       " 909: 'brothers',\n",
       " 910: 'hardly',\n",
       " 911: 'short',\n",
       " 912: 'involved',\n",
       " 913: 'offer',\n",
       " 914: 'mike',\n",
       " 915: 'given',\n",
       " 916: 'killer',\n",
       " 917: 'team',\n",
       " 918: 'stopped',\n",
       " 919: 'join',\n",
       " 920: 'college',\n",
       " 921: 'mark',\n",
       " 922: 'charge',\n",
       " 923: 'silly',\n",
       " 924: 'de',\n",
       " 925: 'nah',\n",
       " 926: 'girlfriend',\n",
       " 927: 'attack',\n",
       " 928: 'pass',\n",
       " 929: 'reach',\n",
       " 930: 'forgive',\n",
       " 931: 'holy',\n",
       " 932: 'bathroom',\n",
       " 933: 'awful',\n",
       " 934: 'history',\n",
       " 935: 'paper',\n",
       " 936: 'smell',\n",
       " 937: 'future',\n",
       " 938: 'fly',\n",
       " 939: 'movies',\n",
       " 940: 'finally',\n",
       " 941: 'joke',\n",
       " 942: 'standing',\n",
       " 943: 'pain',\n",
       " 944: 'service',\n",
       " 945: 'holding',\n",
       " 946: 'carry',\n",
       " 947: 'doc',\n",
       " 948: 'ted',\n",
       " 949: 'present',\n",
       " 950: 'imagine',\n",
       " 951: 'security',\n",
       " 952: 'bus',\n",
       " 953: 'wedding',\n",
       " 954: 'jail',\n",
       " 955: 'smoke',\n",
       " 956: 'everythings',\n",
       " 957: 'station',\n",
       " 958: 'putting',\n",
       " 959: 'park',\n",
       " 960: 'cute',\n",
       " 961: 'detective',\n",
       " 962: 'middle',\n",
       " 963: 'birthday',\n",
       " 964: 'cost',\n",
       " 965: 'saved',\n",
       " 966: 'become',\n",
       " 967: 'dunno',\n",
       " 968: 'deep',\n",
       " 969: 'cover',\n",
       " 970: 'third',\n",
       " 971: 'machine',\n",
       " 972: 'plenty',\n",
       " 973: 'reading',\n",
       " 974: 'grand',\n",
       " 975: 'louis',\n",
       " 976: 'born',\n",
       " 977: 'won',\n",
       " 978: 'church',\n",
       " 979: 'fat',\n",
       " 980: 'sleeping',\n",
       " 981: 'usually',\n",
       " 982: 'master',\n",
       " 983: 'computer',\n",
       " 984: 'orders',\n",
       " 985: 'island',\n",
       " 986: 'sing',\n",
       " 987: 'keeping',\n",
       " 988: 'betty',\n",
       " 989: 'mention',\n",
       " 990: 'enjoy',\n",
       " 991: 'bridge',\n",
       " 992: 'twice',\n",
       " 993: 'correct',\n",
       " 994: 'promised',\n",
       " 995: 'lead',\n",
       " 996: 'south',\n",
       " 997: 'aw',\n",
       " 998: 'space',\n",
       " 999: 'notice',\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_decoder)):\n",
    "    x_decoder[i] = \"<go> \" + x_decoder[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_)):\n",
    "    y_[i] += \" <eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<go> lay off asshole', 'lay off asshole <eos>')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_decoder[3000], y_[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing to sequences\n",
    "# the words occuring in sentences that are not present in vocab are replace by unk\n",
    "X_encoder = []\n",
    "X_decoder = []\n",
    "Y = []\n",
    "\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for x in x_encoder:\n",
    "    tokens = []\n",
    "    for word in x.split():\n",
    "        if word not in word2idx:\n",
    "            tokens.append(word2idx['<unk>'])\n",
    "            unk_count += 1\n",
    "        else:\n",
    "            tokens.append(word2idx[word])\n",
    "            word_count += 1\n",
    "    X_encoder.append(tokens)\n",
    "    \n",
    "for x in x_decoder:\n",
    "    tokens = []\n",
    "    for word in x.split():\n",
    "        if word not in word2idx:\n",
    "            tokens.append(word2idx['<unk>'])\n",
    "            unk_count += 1\n",
    "        else:\n",
    "            tokens.append(word2idx[word])\n",
    "            word_count += 1\n",
    "    X_decoder.append(tokens)\n",
    "    \n",
    "for y in y_:\n",
    "    tokens = []\n",
    "    for word in y.split():\n",
    "        if word not in word2idx:\n",
    "            tokens.append(word2idx['<unk>'])\n",
    "        else:\n",
    "            tokens.append(word2idx[word])\n",
    "    Y.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.178284142784869"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unk_count / word_count) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "X_encoder = pad_sequences(X_encoder, maxlen=max_len, dtype='int32', padding='post', truncating='post')\n",
    "X_decoder = pad_sequences(X_decoder, maxlen=max_len, dtype='int32', padding='post', truncating='post')\n",
    "Y = pad_sequences(Y, maxlen=max_len, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((167126, 20), (167126, 20), (167126, 20))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoder.shape, X_decoder.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0]),\n",
       " array([  3,  85,   4,  15, 119, 107,   9, 650,  43,   9, 580,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]),\n",
       " array([ 85,   4,  15, 119, 107,   9, 650,  43,   9, 580,   1,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoder[30], X_decoder[30], Y[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping word2idx and idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_idx.pkl', 'wb') as f:\n",
    "    pickle.dump([word2idx, idx2word], f, protocol=4)\n",
    "\n",
    "# loading X and Y\n",
    "with open('word_idx.pkl', 'rb') as f:\n",
    "    word2idx, idx2word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping X, Y to pickle for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_and_y.pkl', 'wb') as f:\n",
    "    pickle.dump([X_encoder, X_decoder, Y], f, protocol=4)\n",
    "\n",
    "# loading X and Y\n",
    "with open('x_and_y.pkl', 'rb') as f:\n",
    "    X_encoder, X_decoder, Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embeddings pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that returns the word vector for a given word (in string) from the dataframe of word vectors obtained in the next cell\n",
    "def get_vector(word):\n",
    "    return words.loc[word].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a pandas dataframe of entries from the glove pre-trained vectors txt file as running a loop for getting word to vector mapping is expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = pd.read_table(\"glove.840B.300d/glove.840B.300d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since it is computationally expensive to make a dataframe from txt each time, dumping the dataframe to a pickle; load from \"words.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping the dataframe\n",
    "#with open('words.pkl', 'wb') as f:\n",
    "#    pickle.dump(words, f, protocol=4)\n",
    "\n",
    "# loading the dataframe\n",
    "with open('words.pkl', 'rb') as f:\n",
    "    words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocab_embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab_embedding = np.zeros((len(word2idx), 300))     # embedding dim is 300\n",
    "\n",
    "for i in idx2word.keys():\n",
    "    try:\n",
    "        temp = get_vector(idx2word[i])\n",
    "        vocab_embedding[i] = temp\n",
    "    except:\n",
    "        print(idx2word[i] + \" not in glove\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump vocab_embedding to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_embedding, f, protocol=4)\n",
    "\n",
    "# loading vocab_embeddings\n",
    "with open('embedding_weights.pkl', 'rb') as f:\n",
    "    vocab_embedding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
